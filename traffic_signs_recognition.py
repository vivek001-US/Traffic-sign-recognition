# -*- coding: utf-8 -*-
"""Traffic Signs Recognition.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1yPIj9h8RLNuTQMFk82ZI3KQUcETUD8Xu
"""

# ✅ STEP 1: Mount Google Drive & Extract Dataset
from google.colab import drive
import zipfile
import os

drive.mount('/content/drive')

zip_path = '/content/drive/MyDrive/GTSRB.zip'  # Make sure this is your uploaded ZIP path
extract_path = '/content/GTSRB'

with zipfile.ZipFile(zip_path, 'r') as zip_ref:
    zip_ref.extractall(extract_path)

print("✅ Dataset Extracted Successfully!")

# ✅ STEP 2: Import Libraries
import cv2
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout

# ✅ STEP 3: Load and Preprocess Images
IMG_SIZE = 32
data_dir = '/content/GTSRB/Train'  # ✅ Your correct folder

data = []
labels = []

for folder in os.listdir(data_dir):
    folder_path = os.path.join(data_dir, folder)
    if not os.path.isdir(folder_path):
        continue
    for img_name in os.listdir(folder_path):
        img_path = os.path.join(folder_path, img_name)
        img = cv2.imread(img_path)
        img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
        data.append(img)
        labels.append(int(folder))

X = np.array(data) / 255.0
y = to_categorical(np.array(labels))
print("✅ Loaded images:", len(X))

# ✅ STEP 4: Split Data
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# ✅ STEP 5: Build CNN Model
model = Sequential([
    Conv2D(32, (3,3), activation='relu', input_shape=(IMG_SIZE, IMG_SIZE, 3)),
    MaxPooling2D(2,2),
    Conv2D(64, (3,3), activation='relu'),
    MaxPooling2D(2,2),
    Flatten(),
    Dense(128, activation='relu'),
    Dropout(0.5),
    Dense(y.shape[1], activation='softmax')
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])
model.summary()

# ✅ STEP 6: Train Model
history = model.fit(X_train, y_train, epochs=10, batch_size=64, validation_split=0.2)

# ✅ STEP 7: Evaluate Model
loss, acc = model.evaluate(X_test, y_test)
print("✅ Test Accuracy:", acc)

from sklearn.metrics import classification_report, confusion_matrix
import seaborn as sns

y_pred = model.predict(X_test)
y_pred_classes = np.argmax(y_pred, axis=1)
y_true = np.argmax(y_test, axis=1)

print(classification_report(y_true, y_pred_classes))
sns.heatmap(confusion_matrix(y_true, y_pred_classes), annot=True, fmt="d")
plt.title("Confusion Matrix")
plt.show()

# ✅ STEP 8: Accuracy Plot
plt.plot(history.history['accuracy'], label='Training Accuracy')
plt.plot(history.history['val_accuracy'], label='Validation Accuracy')
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

# ✅ STEP 9: Save Model to Drive
model.save('/content/drive/MyDrive/traffic_sign_model.h5')
print("✅ Model saved to Google Drive")

# ✅ STEP 10 (Improved): Predict Specific Class Image (e.g., Class 14)
chosen_class = '1'  # <-- You can change this to any class folder like '33', '5', etc.
sample_img_name = os.listdir(os.path.join(data_dir, chosen_class))[0]
img_path = os.path.join(data_dir, chosen_class, sample_img_name)

print("🖼️ Selected image:", img_path)

# Load and preprocess image
img = cv2.imread(img_path)
img = cv2.resize(img, (IMG_SIZE, IMG_SIZE))
img_norm = img / 255.0
img_input = np.expand_dims(img_norm, axis=0)

# Predict
prediction = model.predict(img_input)
predicted_class = np.argmax(prediction)
print("✅ Predicted Class:", predicted_class)

# Show image
img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
plt.imshow(img_rgb)
plt.title(f"Predicted: {predicted_class} | Actual: {chosen_class}")
plt.axis("off")
plt.show()